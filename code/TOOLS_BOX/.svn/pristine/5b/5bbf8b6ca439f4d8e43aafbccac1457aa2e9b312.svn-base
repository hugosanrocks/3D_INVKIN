module comm_engine_mod

  use dd_common_mod
  use mem_alloc_mod
  use grid_mod
  use mpi
  use omp_lib

  implicit none

  public :: init_mpi

  !=========================================================
  type comm_engine_type
    character(len=20), private      :: name = 'comm_engine'

    integer       :: nproc_world = 1
    integer       :: myid_world  = 0

    !-----------------------------------------------------
    ! MPI intra-group communicator mpi_comm_1
    ! for sub-domain communications
    ! associated with myid_1, nproc_1 and mycolor 
    !-----------------------------------------------------
    integer       :: mpi_comm_1  = MPI_COMM_NULL
    integer       :: nproc_1     = 1
    integer       :: myid_1      = 0
    integer       :: mycolor     = 0

    !-----------------------------------------------------
    ! MPI inter-group communicator MPI_COMM_2
    ! for communications between sub-domains of same rank
    ! associated with myid_2 , nproc_2 and mykey
    !-----------------------------------------------------
    integer       :: mpi_comm_2  = MPI_COMM_NULL
    integer       :: nproc_2     = 0
    integer       :: myid_2      = 0
    integer       :: mykey       = 0

  contains

    procedure, pass :: end_mpi
    procedure, pass :: create_mpi_communicators
    procedure, pass :: create_mpi_types
    procedure, pass :: print_info

  end type comm_engine_type
  !=========================================================
  
  contains


  subroutine init_mpi()
    integer                                 :: thread_level_required = 0, thread_level_provided = 0, mpierr
    character                               :: thread_level_provided_str*25
    logical                                 :: already_initialized
    character(len=*),        parameter      :: proc_name = 'init_mpi'

    if (dd_debug_level > 3) then
      write(*,*) proc_name
    endif

    ! Check MPI not already initialized
    call mpi_initialized(already_initialized, mpierr)
    if (mpierr /= 0) then
      print *, proc_name, " :: ERROR : mpi_initialized ==", mpierr
      stop
    end if
    !print *, proc_name, " initialized ==", already_initialized

    if (nthreads < 0) then
      nthreads = omp_get_num_threads()
    end if

    if (.not. already_initialized) then

      if (nthreads == 0) then

        call mpi_init(mpierr)

        if (mpierr /= 0) then
          print *, proc_name, " :: ERROR : mpi_init ==", mpierr
          stop
        end if

      else

        ! MPI_THREAD_SINGLE < MPI_THREAD_FUNNELED < MPI_THREAD_SERIALIZED < MPI_THREAD_MULTIPLE
        thread_level_required = MPI_THREAD_MULTIPLE

        call mpi_init_thread(thread_level_required, thread_level_provided, mpierr)

        if (mpierr /= 0) then
          print *, proc_name, " :: ERROR : mpi_init ==", mpierr
          stop
        end if

        select case(thread_level_provided)
        case(MPI_THREAD_SINGLE)
          thread_level_provided_str = "MPI_THREAD_SINGLE"
        case(MPI_THREAD_FUNNELED)
          thread_level_provided_str = "MPI_THREAD_FUNNELED"
        case(MPI_THREAD_SERIALIZED)
          thread_level_provided_str = "MPI_THREAD_SERIALIZED"
        case(MPI_THREAD_MULTIPLE)
          thread_level_provided_str = "MPI_THREAD_MULTIPLE"
        end select
!         if ((myid == 0) .and. (thread_level_required .ne. thread_level_provided)) then
!           print *, proc_name, " :: WARNING : provided =", thread_level_provided_str, &
!           &                              " < required =", thread_level_required_str
!           !call mpi_finalize(mpierr)
!         end if

      end if

    end if

    call mpi_comm_size(MPI_COMM_WORLD, nproc_world, mpierr)
    call mpi_comm_rank(MPI_COMM_WORLD, myid_world , mpierr)
    myid = myid_world !temporary affectation before communicator splitting for message printing while configuration reading

    if (myid_world == 0) then
      write (*,*)
      write (*,*) "MPI is initialized"
      write (*,*) "Executable launched on :"
      if (nproc_world > 1)  then
        write (*,*) " - ", nproc_world, " processors"
      else
        write (*,*) " - A single processor"
      end if
      if (nthreads > 1) then
        write (*,*) " - ", nthreads, " threads (MPI thread support level = ", thread_level_provided_str, ")"
      else
        write (*,*) " - A single thread"
      end if
      write (*,*)
    end if

  end subroutine init_mpi


  subroutine end_mpi(this)
    class(comm_engine_type), intent(in out) :: this
    integer                                 :: mpierr
    character(len=*),        parameter      :: proc_name = 'end_mpi'

    if (dd_debug_level > 3) then
      write(*,*) myid, ' : ', proc_name
    endif

    call mpi_barrier(this%mpi_comm_1, mpierr)
    if (mpierr < 0) then
      print *, proc_name, ' :: ERROR : mpi_barrier: ', mpierr
      stop
    end if

    if (this%mpi_comm_1 /= MPI_COMM_NULL) then
      call mpi_comm_free(this%mpi_comm_1, mpierr)
      if (mpierr < 0) then
        print *, proc_name, ' :: ERROR : mpi_comm_free: ', mpierr
        stop
      end if
    end if

    if (this%mpi_comm_2 /= MPI_COMM_NULL) then
      call mpi_comm_free(this%mpi_comm_2, mpierr)
      if (mpierr < 0) then
      print *, proc_name, ' :: ERROR : mpi_comm_free: ', mpierr
      stop
      end if
    end if

    call mpi_finalize(mpierr)
    if (mpierr < 0) then
      print *, proc_name, ' :: ERROR : mpi_finalize: ', mpierr
      stop
    end if

  end subroutine end_mpi


  subroutine create_mpi_communicators(this)

    !----------------------------------------------------------------------
    !     split MPI processes according to 
    !     - the nb of sources computed in parallel 
    !     - and the number of sub-domains
    !
    !     Create new MPI communicator       -> mpi_comm_1
    !            new rank id                -> myid_1
    !            new nb of proc in new comm -> nproc_1
    !
    !     Create new MPI communicator       -> mpi_comm_2
    !            new rank id                -> myid_2   
    !            new nb of proc in new comm -> nproc_2
    !----------------------------------------------------------------------

    class(comm_engine_type), intent(in out) :: this
    integer                                 :: igroup, ipart, iproc, mpierr
    integer, dimension(:),   allocatable    :: rank, color
    character(len=*),        parameter      :: proc_name = 'create_mpi_communicators'

    if (dd_debug_level > 3) then
      write(*,*) this%myid_world, ' : ', proc_name
      call mpi_barrier(MPI_COMM_WORLD, mpierr)
    endif

    this%nproc_world = nproc_world
    this%myid_world  = myid_world
    nsrc_par         = nproc_world / npart

    ! allocate tables used in mpi_comm_split
    call alloc_(color, 1, this%nproc_world, proc_name // "/color")
    call alloc_(rank,  1, this%nproc_world, proc_name // "/rank")

    !------------------
    ! Create MPI_COMM_1
    !------------------
    
    iproc = 0

    ! loop on the nb of sources in parallel
    do igroup = 1, nsrc_par

      ! loop on the nb of sub-domains
      do ipart = 1, npart

          iproc = iproc + 1
          color(iproc) = igroup
          rank (iproc) = ipart - 1

      enddo
    enddo

    if (dd_debug_level > 3) then
      write(*,*) 'color', color
      write(*,*) 'rank ', rank
    endif

    this%mycolor = color(this%myid_world+1)

    ! split mpi_comm_world into mpi_comm_1 (nb groups = nsrc_par)
    call mpi_comm_split(MPI_COMM_WORLD, color(this%myid_world+1), rank(this%myid_world+1), this%mpi_comm_1, mpierr)

    ! retrieve myid and nproc for communicator 1
    call mpi_comm_rank( this%mpi_comm_1, this%myid_1,  mpierr )
    call mpi_comm_size( this%mpi_comm_1, this%nproc_1, mpierr )

    if (this%nproc_1 /= npart) then
      write(*,*) proc_name, " :: ERROR : nproc_1 /= npart :", this%nproc_1, npart
      call flush(6)
      stop
    end if

    myid = this%myid_1 !myid : global var used for message printing

    !------------------
    ! Create MPI_COMM_2
    !------------------

    color = 0
    rank  = 0
    iproc = 0

    ! loop on the nb of sources in parallel
    do igroup = 1, nsrc_par

      ! loop on the nb of sub-domains
      do ipart = 1, npart

        iproc = iproc + 1
        color(iproc) = ipart
        rank(iproc)  = igroup-1

      end do
    end do

    if (dd_debug_level > 1 .and. this%myid_world == 0) then
      write(*,*) 'color', color
      write(*,*) 'rank ', rank
    endif

    this%mykey = color(this%myid_world+1)

    ! split mpi_comm_world into mpi_comm_2 (nb groups = nsrc_par)
    call mpi_comm_split(MPI_COMM_WORLD, color(this%myid_world+1), rank(this%myid_world+1), this%mpi_comm_2, mpierr)

    ! retrieve myid and nproc for communicator 2
    call mpi_comm_rank( this%mpi_comm_2, this%myid_2,  mpierr )
    call mpi_comm_size( this%mpi_comm_2, this%nproc_2, mpierr )

    if (this%nproc_2 /= nsrc_par) then
      write(*,*) proc_name, " :: ERROR : nproc_2 /= nsrc_par :", this%nproc_2, nsrc_par
      call flush(6)
      stop
    end if

    call dealloc_(rank,  proc_name // "/rank")
    call dealloc_(color, proc_name // "/color")

    if (dd_debug_level > 1) then
      write(*,*) ' nproc_world, nproc_1, nproc_2 ', this%nproc_world, this%nproc_1, this%nproc_2
      write(*,*) ' myid_world,  myid_1,  myid_2  ', this%myid_world,  this%myid_1,  this%myid_2
    endif

  end subroutine create_mpi_communicators


  subroutine create_mpi_types(this, grid)
    class(comm_engine_type),  intent(in out) :: this
    class(grid_type),         intent(in)     :: grid
    character(len=50)                        :: proc_name
    proc_name = trim(this%name) // '%create_mpi_types'
    write(*,*) trim(proc_name), ' :: ERROR : Virtual method invoked on :', trim(this%name)
    stop
  end subroutine create_mpi_types


  subroutine print_info(this)
    class(comm_engine_type), intent(in out) :: this
    character(len=*),        parameter      :: proc_name = 'print_info'
    write(*,*) ' Effective number of processes : ', this%nproc_world
  end subroutine print_info


end module comm_engine_mod
  