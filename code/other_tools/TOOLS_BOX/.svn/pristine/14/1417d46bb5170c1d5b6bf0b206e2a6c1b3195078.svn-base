program test_dd_sg_ac_iso_o4

  use dd_common_mod, only : dd_debug_level
  use mem_alloc_mod
  use sgrid_mod
  use sg_comm_mod
  use sg_points_zone_mod

  implicit none

  type(sgrid_type)                     :: grid
  type(sg_comm_type)                   :: comm

  ! input parameters
  integer, dimension(3)                :: nglobal, nsub_dom
  integer, dimension(2,3)              :: npml_glob
  real,    dimension(3)                :: spatial_step, grid_origin
  integer                              :: nsrc_comm1, nt
  real                                 :: dt
  real                                 :: vp_val, rho_val
  character(len=100)                   :: vp_file = 'vp', rho_file = 'rho'
  real                                 :: pml_vmax, pml_Rc,  pml_freq, pml_kmax
  integer                              :: pml_power
  real                                 :: ricker_freq 
  real                                 :: ricker_amp

  ! Stress and velocity fields solution
  real, dimension(:,:,:),  allocatable :: Vx, Vy, Vz, P, Pglob

  ! Physical parameters
  real, dimension(:,:,:),  allocatable :: tmp_glob
  real, dimension(:,:,:),  allocatable :: Vp, rho
  real, dimension(:,:,:),  allocatable :: buo_Vx, buo_Vy, buo_Vz, kappa

  ! PML memory variables
  real, dimension(:,:,:),  allocatable :: mem_Vx_x, mem_Vy_y, mem_Vz_z
  real, dimension(:,:,:),  allocatable :: mem_Px_x, mem_Py_y, mem_Pz_z

  ! PML coefficients
  real, dimension(:),      allocatable :: az, ax, ay, az_half, ax_half, ay_half
  real, dimension(:),      allocatable :: bz, bx, by, bz_half, bx_half, by_half
  real, dimension(:),      allocatable :: kz, kx, ky, kz_half, kx_half, ky_half

  ! Sources location
  integer                              :: nsrc, nsrc_loc
  real, dimension(:,:),    allocatable :: src_coor
  integer, dimension(:,:), allocatable :: src_loc
  integer, dimension(:),   allocatable :: src_comm1

  ! Receivers location
  integer                                  :: nrec_glob
  real, dimension(:,:),        allocatable :: rec_glob_coor
  type(sg_continuum_points_glob_zone_type) :: rec_glob_zone
  type(sg_continuum_points_loc_zone_type)  :: rec_loc_zone
  real, dimension(:,:),        allocatable :: rec_glob_buf_sol, rec_loc_buf_sol

  ! source time function defined as a ricker wavelet
  real, dimension(:),      allocatable :: source

  integer                              :: it, ierr, isrc_glob, isrc_comm1, iseismo, irec
  integer                              :: novlp, ntmpblock, npml_loc
  real                                 :: time_end   = 0.
  real                                 :: time_begin = 0.

  dd_debug_level = 0 ! declared in common_mod

  iseismo = 1

  call init_mpi()

  call read_input_parameters()

  call grid%sgrid_constructor()
  call grid%sgrid_set_parameters(nglobal, nsub_dom, npml_glob, spatial_step, grid_origin)

  call comm%create_mpi_communicators()

  call dispatch_sources_on_comm2(comm, nsrc, nsrc_comm1, src_comm1)

  ! For the self-consistency of the test, create parameter files and source funtion on the fly
  call build_global_homogeneous_parameter(vp_file,  vp_val,  nglobal)
  call build_global_homogeneous_parameter(rho_file, rho_val, nglobal)
  call mpi_barrier(mpi_comm_world,ierr)
  call build_source_excitation_term(source, nt, dt, spatial_step, ricker_freq, ricker_amp)

  call print_starting_banner()

  if (myid == 0) then
    do irec = 1, nrec_glob
      write(*,*) 'Receiver ', irec, ' coor =', rec_glob_coor(:,irec)
    end do
    call flush(6)
  end if

  ! Init domain decomposition------------------------------
  call grid%sgrid_set_diag_neighbours(.False.) ! True if communication with diagonal neighbours processors is needed
  call grid%grid_init()
  novlp = get_scheme_overlap()
  call grid%grid_set_overlap(novlp)
  call grid%grid_set_temporal_blocking_size(ntmpblock)
  !--------------------------------------------------------

  ! Create MPI types for communications--------------------
  call comm%create_mpi_types(grid)
  !--------------------------------------------------------

  call alloc_wavefields()
  call grid%sgrid_allocate_global_field(Pglob, "Pglob")

  call cpml_init(grid, dt, pml_vmax, pml_Rc, pml_freq, pml_kmax, pml_power, npml_loc)

  ! Read, allocate and build physical parameters-----------
  call grid%sgrid_allocate_global_field(tmp_glob, "tmp_glob")

  ! Vp
  call grid%sgrid_allocate_field(Vp, "Vp")
  call grid%sgrid_read_global_field(tmp_glob, vp_file)
  call comm%sg_comm_scatter_global_field(grid, tmp_glob, Vp)

  ! Rho
  call grid%sgrid_allocate_field(rho, "rho")
  call grid%sgrid_read_global_field(tmp_glob, rho_file)
  call comm%sg_comm_scatter_global_field(grid, tmp_glob, rho)

  call grid%sgrid_deallocate_global_field(tmp_glob, "tmp_glob")
  call build_physical_parameters(grid, comm, vp, rho, buo_Vx, buo_Vy, buo_Vz, kappa)
  !--------------------------------------------------------

  call find_src(grid, src_coor, nsrc_comm1, src_comm1, nsrc_loc, src_loc)

  call distribute_receivers_on_subdomains()

  call cpu_time(time_begin)

  do isrc_comm1 = 1, nsrc_comm1

    ! retrieve global src index
    isrc_glob = src_comm1(isrc_comm1)

    call reset_wavefields()

    do it = 1, nt

      if (myid == 0 .and. (nt <= 10 .or. (nt > 10 .and. mod(it, nt/10) == 0))) then
        write(*,*)
        write(*,*) ' current iteration ', it, '/', nt
      end if

      ! Communicate velocity-------------------------------
      call comm%sg_comm_communicate_field(Vx, grid)
      call comm%sg_comm_communicate_field(Vy, grid)
      call comm%sg_comm_communicate_field(Vz, grid)
      !----------------------------------------------------

      call compute_stress()
      call compute_source_stress(source, it, isrc_glob, nsrc_loc, src_loc)

      ! Communicate stress---------------------------------
      call comm%sg_comm_communicate_field(P, grid)
      !----------------------------------------------------

      call compute_velocity()

      if ((nt > 10 .and. mod(it, nt/10) == 0) .or. nt <= 10) then
        ! Gather P field on the master processor for output
        call comm%sg_comm_gather_global_field(grid, Pglob, P)
        if (myid == 0) then
          call fdm_write_vtk_legacy_fmt(grid, isrc_glob, it, Pglob, "Pglob")
        end if
      end if

      call extract_sol_at_receivers()

    end do ! it = 1, nt

    call write_sol_at_receivers()

  end do ! do isrc_comm1 = 1, nsrc

  call cpu_time(time_end)
  if (myid == 0) then
    write(*,*)
    write(*,*) '--------------------------------------------------------------------'
    write(*,*) ' Total time (seconds) = ', REAL(time_end - time_begin)
    write(*,*) '--------------------------------------------------------------------'
  end if

  call print_statistics()

  ! Free memory--------------------------------------------
  call free_mem()
  call grid%grid_destructor()
  if (myid == 0) then
    call dealloc_(rec_glob_buf_sol, 'rec_glob_buf_sol')
    call rec_glob_zone%glob_zone_destructor()
  end if
  call dealloc_(rec_loc_buf_sol, 'rec_loc_buf_sol')
  call rec_loc_zone%loc_zone_destructor()
  call print_mem_alloc_stat()
  call comm%end_mpi()
  !--------------------------------------------------------

  call print_successfully_program_ended_banner()

  contains


  subroutine read_input_parameters()
    integer :: plan, n1, n2, i, j, isrc, irec
    real    :: d1, d2, x0, y0, z0

    open(unit=1, FILE='test_dd_sg_ac_iso_o4.config')

    ! domain and sources distributions on processor
    read(1,*)   nsub_dom

    ! grid
    read(1,*)   nglobal
    read(1,*)   spatial_step
    read(1,*)   grid_origin
    read(1,*)   npml_glob
    read(1,*)   nt
    read(1,*)   dt
    read(1,*)   ntmpblock

    ! parameters for input physical param
    read(1,*)   vp_val
    read(1,*)   rho_val

    ! parameters for source time function
    read(1,*)   ricker_freq! frequency of the ricker wavelet
    read(1,*)   ricker_amp! maximum amplitude of the ricker wavelet

    ! parameters for cpml
    read(1,*)   pml_vmax
    read(1,*)   pml_Rc
    read(1,*)   pml_freq
    read(1,*)   pml_kmax
    read(1,*)   pml_power

    ! sources parameters
    read(1,*)   nsrc
    call alloc_(src_coor, 1, nsrc, 1, 3, "src_coor")
    do isrc = 1, nsrc
      read(1,*) src_coor(isrc,:)
    end do

    ! receivers parameters
    read(1,*)   plan          ! plan (1=yz,2=xz,3=xy)
    read(1,*)   x0, y0, z0    ! xmin, ymin, zmin
    read(1,*)   n1, d1        ! nb points, delta between points for axis 1
    read(1,*)   n2, d2        ! nb points, delta between points for axis 2
    nrec_glob = n1 * n2
    call alloc_(rec_glob_coor, 1, 3, 1, nrec_glob, "rec_coor")
    irec = 0
    do i = 0, n1-1
      do j = 0, n2-1
          irec = irec + 1
          if (plan == 1) then
            rec_glob_coor(1,irec) = x0 
            rec_glob_coor(2,irec) = y0 + i*d1
            rec_glob_coor(3,irec) = z0 + j*d2
          else if (plan == 2) then 
            rec_glob_coor(1,irec) = x0 + i*d1
            rec_glob_coor(2,irec) = y0 
            rec_glob_coor(3,irec) = z0 + j*d2
          else if (plan == 3) then
            rec_glob_coor(1,irec) = x0 + i*d1
            rec_glob_coor(2,irec) = y0 + j*d2
            rec_glob_coor(3,irec) = z0 
          end if
      end do
    end do

    close(unit=1)
  end subroutine read_input_parameters


  subroutine build_global_homogeneous_parameter(filename, value, nglobal)
    ! This subroutine stores on file an homogeneous parameter value on the global grid

    integer, dimension(3), intent(in) :: nglobal
    real,                  intent(in) :: value
    character(len=100),    intent(in) :: filename
    integer                           :: i, j, k

    if (myid == 0) then
      write(6,*) '   build_homogeneous_param ', trim(adjustl(filename)), value

      open (unit=11, file=trim(adjustl(filename)), access='direct', form='unformatted', status='unknown', &
      & recl=product(nglobal)*kind(value))
      write(unit=11, rec=1) (((value, i=1,nglobal(1)), j=1,nglobal(2)), k=1,nglobal(3))
      close(unit=11)
    end if
  end subroutine build_global_homogeneous_parameter


  subroutine dispatch_sources_on_comm2(comm, nsrc, nsrc_comm1, src_comm1)
    type(sg_comm_type),                 intent(in)  :: comm
    integer,                            intent(out) :: nsrc_comm1
    integer,                            intent(in)  :: nsrc
    integer, dimension(:), allocatable, intent(out) :: src_comm1
    integer                                         :: isrc, jsrc, nsrc_tmp

    ! initialize the nb of source associated to MPI_COMM1
    nsrc_comm1 = 0

    nsrc_tmp = ceiling(real(nsrc) / real(nsrc_par)) 
    if (nsrc_tmp < 1) then
      write(*,*) 'nsrc_tmp =', nsrc_tmp
      stop
    end if
    call alloc_(src_comm1, 1, nsrc_tmp, "src_comm1")

    jsrc = 0
    do isrc = 1, nsrc
      if (jsrc == nsrc_par) then
        jsrc = 1
      else
        jsrc = jsrc + 1
      end if

      ! keep in list global index of source
      if (comm%mycolor == jsrc) then
        nsrc_comm1 = nsrc_comm1 + 1
        src_comm1(nsrc_comm1) = isrc
      end if
    end do

  end subroutine dispatch_sources_on_comm2


  subroutine find_src(grid, src_coor, nsrc_comm1, src_comm1, nsrc_loc, src_loc)
    type(sgrid_type),                     intent(in)  :: grid
    integer,                              intent(in)  :: nsrc_comm1
    real, dimension(:,:), allocatable,    intent(in)  :: src_coor
    integer, dimension(:), allocatable,   intent(in)  :: src_comm1
    integer,                              intent(out) :: nsrc_loc
    integer, dimension(:,:), allocatable, intent(out) :: src_loc

    !     Retrieve source positions in the staggered grid
    !     and build tables according to the nb of sources located in the subdomain

    integer                                        :: isrc, isrc_comm1
    integer, dimension(3)                          :: iglob, iloc

    !------------------------------------------------------------------
    ! 1st loop on the sources
    ! to retrieve the nb of sources in subdomain
    !------------------------------------------------------------------

    nsrc_loc = 0

    do isrc_comm1 = 1, nsrc_comm1 

      ! retrieve global src index
      isrc = src_comm1(isrc_comm1) 

      ! check if src belongs or intersects the subdomain
      if (grid%sgrid_is_point_inside_sub_domain(src_coor(:,isrc), 0, iglob, iloc)) then
        nsrc_loc = nsrc_loc + 1 
      endif

    end do

    if (nsrc_loc > 0) then
      write(*,*) myid, ' : nsrc_loc ', nsrc_loc
    end if

    !-----------------------------
    ! allocate local source table
    !-----------------------------

    if (nsrc_loc > 0) then
      call alloc_(src_loc, 1, nsrc_loc, 1, 4, "src_loc")
    endif

    !------------------------------------------------------------------
    ! 2nd loop on the sources
    ! to fill the source table
    !------------------------------------------------------------------

    nsrc_loc = 0

    do isrc_comm1 = 1, nsrc_comm1 

      ! retrieve global src index
      isrc = src_comm1(isrc_comm1) 

      ! check if src belongs or intersects the subdomain
      if (grid%sgrid_is_point_inside_sub_domain(src_coor(:,isrc), 0, iglob, iloc)) then

        nsrc_loc = nsrc_loc + 1 
  
        ! fill index and coordinates in the table
        !=========================================

        src_loc(nsrc_loc, 1)   = isrc     ! source global index
        src_loc(nsrc_loc, 2:4) = iloc(:)  ! source local indices coordinates

      end if

    end do

  end subroutine find_src


  subroutine distribute_receivers_on_subdomains()
    integer   :: iproc, i1, iEnd, ipt, i
    character :: buffer*10000, str1*4

    ! Define global zone on master processor only
    ! (in fact each master processor of communicator 1 dedicated to domain decomposition)
    if (myid == 0) then
      write(*,*)
      write(*,*) 'Distribute receivers on subdomains ...'
      write(*,*)

      call rec_glob_zone%sg_continuum_points_glob_zone_constructor(nrec_glob, rec_glob_coor, 0)

    end if

    ! Perform partition of the global zone, return a local zone on each subdomain
    call comm%sg_comm_partition_sg_continuum_points_global_zone(grid, rec_glob_zone, rec_loc_zone)

! BEGIN outputs for debug ------------- 
    ! print global zone data
    if (myid == 0) then
      if (rec_glob_zone%npt_glob > 0) then
        write(buffer,'(3A,33A1,3A)') &
        & end_of_line, ' RECEIVERS GLOBAL ZONE PARTITION:', end_of_line, ('-', i=1, 33), end_of_line, &
        & ' Proc -> Pt table:', end_of_line
        do iproc = -1, comm%nproc_1-1
          i1   = rec_glob_zone%proc_npt_tab(iproc)
          iEnd = rec_glob_zone%proc_npt_tab(iproc+1) - 1
          if (iEnd - i1 >= 0) then
            write(str1,'(I4)') iEnd - i1 + 1
            write(buffer,'(2A,I4,A,'// trim(adjustl(str1)) //'I4,A)') &
            & trim(adjustl(buffer)), ' proc = ', iproc, ' proc_pt_tab = ', rec_glob_zone%proc_pt_tab(i1:iEnd), end_of_line
          else
            write(buffer,'(2A,I4,2A)') &
            & trim(adjustl(buffer)), ' proc = ', iproc, ' proc_pt_tab = none', end_of_line
          end if
        end do
        write(*,'(A,A)') trim(adjustl(buffer)), end_of_line
      end if
      write(str1,'(I4)') rec_glob_zone%npt_glob
      write(*,'(A,' // str1 // 'I4,A)') ' Proc -> Pt table: pt_proc_tab = ', rec_glob_zone%pt_proc_tab, end_of_line
    end if

    ! print local zone data
    write(buffer,'(A,I4,A,I4,A,41A1,A)') &
    & end_of_line, myid, ' RECEIVERS LOCAL ZONE: npt_loc = ', rec_loc_zone%npt_loc, end_of_line, ('-', i=1, 41), end_of_line
    do ipt = 1, rec_loc_zone%npt_loc
      write(buffer,'(2A,I4,A,I4,A,3F8.1,A,3I4,A)') &
      & trim(adjustl(buffer)), ' ipt_loc : ', ipt, &
      & ' ipt_glob = ', rec_loc_zone%pts(ipt),     &
      & ' coor = ', rec_loc_zone%coor(:,ipt),      &
      & ' mapping_loc_grid = ', rec_loc_zone%mapping_loc_grid(:,ipt), end_of_line
    end do
    write(*,'(A,A)') trim(adjustl(buffer)), end_of_line
! END outputs for debug ------------- 

    call dealloc_(rec_glob_coor, 'rec_glob_coor')

  end subroutine distribute_receivers_on_subdomains


  subroutine extract_sol_at_receivers()
    integer :: irec, ixr, iyr, izr, nrec_loc

    nrec_loc = rec_loc_zone%npt_loc

    if (nrec_loc > 0) then

      if (.not. allocated(rec_loc_buf_sol)) then
        call alloc_(rec_loc_buf_sol, 1, nt, 1, nrec_loc, "/rec_loc_buf_sol")
      end if

      ! Extract time solutions at the receiver locations

      do irec = 1, nrec_loc

        ! retrieve local coordinates of the receiver
        ixr = rec_loc_zone%mapping_loc_grid(1, irec) ! ixmin
        iyr = rec_loc_zone%mapping_loc_grid(2, irec) ! iymin
        izr = rec_loc_zone%mapping_loc_grid(3, irec) ! izmin

        ! update time solution
        ! store current time step
        rec_loc_buf_sol(it, irec) = P(ixr, iyr, izr)

      end do ! do irec = 1, nrec_loc

    end if ! nrec_loc > 0

  end subroutine extract_sol_at_receivers


  subroutine write_sol_at_receivers()
    character :: filename*10!, fmt*20, nstr*8
    integer   :: nrec_glob, ierr, l_rec!, it, irec
    integer   :: fileunit = 13

    if (myid == 0) then
      if (.not. allocated(rec_glob_buf_sol)) then
        call alloc_(rec_glob_buf_sol, 1, nt, 1, rec_glob_zone%npt_glob, "rec_glob_buf_sol")
      end if
    end if

    ! Gather data
    call comm%sg_comm_gather_points_global_zone_buf_data( &
    & nt, rec_glob_zone, rec_glob_buf_sol, rec_loc_zone, rec_loc_buf_sol)

    ! write file
    if (myid == 0) then

      nrec_glob = rec_glob_zone%npt_glob
      filename = 'P'

      !binary file
      l_rec = nt * nrec_glob * 4
      if (l_rec > 0) then
        open(unit=fileunit, file=trim(adjustl(filename)), access='direct', form='unformatted', &
        &    status='unknown', recl=l_rec, iostat=ierr)
        if (ierr /= 0) then
          write(*,*) "ERROR : could not open file : ", filename, " unit = ", fileunit
          stop
        endif

        write(fileunit, rec=1) real(rec_glob_buf_sol(1:nt,1:nrec_glob),4)

        close(unit = fileunit)
      end if

!       ! ascii file
!       open(unit=fileunit, file=trim(filename)//'.dat', form='formatted', status='unknown', position='append', iostat=ierr)
!       if (ierr /= 0) then
!         write(*,*) "ERROR : could not open file : ", filename//'.dat', " unit = ", fileunit2
!         stop
!       endif
! 
!       write(nstr,'(I8)') nrec_glob
!       write(fmt,'(A)') '(' // trim(adjustl(nstr)) // 'E20.12)'
! 
!       do it = 1, nt
!           ! absolute position in file
!           irec   = it + (nt * (isrc_glob - 1)) 
!           write(fileunit, fmt) real(rec_glob_buf_sol(it,1:nrec_glob),4)
!       end do
! 
!       close(unit = fileunit)

    end if ! myid == 0

  end subroutine write_sol_at_receivers


  subroutine build_physical_parameters(grid, comm, vp, rho, buo_Vx, buo_Vy, buo_Vz, kappa)
    real, dimension(:,:,:), allocatable, intent(in out) :: vp, rho
    real, dimension(:,:,:), allocatable, intent(in out) :: buo_Vx, buo_Vy, buo_Vz, kappa
    integer                                             :: n1loc, n2loc, n3loc
    type(sgrid_type),                    intent(in out) :: grid
    type(sg_comm_type),                  intent(in out) :: comm

    call grid%sgrid_allocate_field(buo_Vx, "buo_Vx")
    call grid%sgrid_allocate_field(buo_Vy, "buo_Vy")
    call grid%sgrid_allocate_field(buo_Vz, "buo_Vz")
    call grid%sgrid_allocate_field(kappa,  "kappa")

    ! interpolation of rho for the nodes located on the staggered grid
    call grid%sgrid_1d_field_interpol_arith_mean(rho, buo_Vx, buo_Vy, buo_Vz)

    n1loc = grid%nnodes_loc(1)
    n2loc = grid%nnodes_loc(2)
    n3loc = grid%nnodes_loc(3)

    ! take inverse to obtain the buoyancy
    buo_Vx(1:n1loc,1:n2loc,1:n3loc) = 1. / buo_Vx(1:n1loc,1:n2loc,1:n3loc)
    buo_Vy(1:n1loc,1:n2loc,1:n3loc) = 1. / buo_Vy(1:n1loc,1:n2loc,1:n3loc)
    buo_Vz(1:n1loc,1:n2loc,1:n3loc) = 1. / buo_Vz(1:n1loc,1:n2loc,1:n3loc)

    call comm%sg_comm_communicate_field(buo_Vx, grid)
    call comm%sg_comm_communicate_field(buo_Vy, grid)
    call comm%sg_comm_communicate_field(buo_Vz, grid)

    ! compute kappa (rho * vp^2)
    kappa = (vp**2) * rho

    call comm%sg_comm_communicate_field(kappa, grid)

  end subroutine build_physical_parameters

  
  subroutine build_source_excitation_term(source, nt, dt, spatial_step, ricker_freq, ricker_amp)
    real, dimension(:), allocatable, intent(out) :: source
    integer,                         intent(in)  :: nt
    real, dimension(3),              intent(in)  :: spatial_step
    real,                            intent(in)  :: dt, ricker_freq, ricker_amp
    integer                                      :: ii
    real                                         :: t0, a1, ttime
    real, parameter                              :: pi = acos(-1.)

    ! This subroutine computes a time function of a ricker wavelet

    if (myid == 0) write(6,*) ' frequency of ricker_ricker ', ricker_freq

    ! allocate table
    call alloc_(source, 1, nt, 'source')

    t0 = 1.5 * sqrt(6.)/(pi * ricker_freq) 
    ttime = 0.

    do ii = 1 , nt  
      a1 = (pi * ricker_freq * (ttime - t0))**2
      source(ii) = ricker_amp * (1. - 2. * a1) * exp(-a1)
      ttime = ttime + dt
    end do

    ! write output file in ascii
    open(unit = 11, file='ricker.ascii', form='formatted', status ='unknown')
    do ii=1,nt
      write(11,*) source(ii)
    enddo
    close(11)

    ! Normalization
    source(:) = source(:) / product(spatial_step(:))

    if (myid == 0) write(*,*) ' max amp', maxval(abs(source(:)))

  end subroutine build_source_excitation_term


  subroutine compute_source_stress(source, it, isrc_glob, nsrc_loc, src_loc)
    real,    dimension(:),   allocatable, intent(in) :: source
    integer,                              intent(in) :: it, isrc_glob
    integer,                              intent(in) :: nsrc_loc
    integer, dimension(:,:), allocatable, intent(in) :: src_loc
    integer                                          :: isrc, izs, ixs, iys
    ! Apply the source term on the stress components

    ! loop on the source located in the subdomain
    do isrc = 1, nsrc_loc

      ! check source index
      if (src_loc(isrc, 1) /= isrc_glob) cycle

      ! retrieve source local coordinates
      izs = src_loc(isrc, 2)
      ixs = src_loc(isrc, 3)
      iys = src_loc(isrc, 4)

      P(izs, ixs, iys) = P(izs, ixs, iys) + source(it) * dt

    end do ! do isrc = 1, nsrc_loc

  end subroutine compute_source_stress


  subroutine cpml_init(grid, dt, vmax, Rc, freq, kmax, power, npml_loc)
    type(sgrid_type),             intent(in) :: grid
    real,                         intent(in) :: dt, vmax, Rc, freq, kmax
    integer,                      intent(in) :: power
    integer,                     intent(out) :: npml_loc ! number of points of the current subdomain present in PML layer
    integer                                  :: idim, i, j, k, n1, n2, n3
    integer, dimension(3)                    :: coord_loc, coord_glob, end_left, start_right

    ! compute PML coefficients along Z, X and Y axis
    idim = 1
    call cpml_compute_coef_on_axis(idim, grid, dt, vmax, Rc, freq, kmax, power, az, bz, kz, az_half, bz_half, kz_half)

    idim = 2
    call cpml_compute_coef_on_axis(idim, grid, dt, vmax, Rc, freq, kmax, power, ax, bx, kx, ax_half, bx_half, kx_half)

    idim = 3
    call cpml_compute_coef_on_axis(idim, grid, dt, vmax, Rc, freq, kmax, power, ay, by, ky, ay_half, by_half, ky_half)

    ! Retrieve the nb of points of the current subdomain that are inside the PML layer
    n1 = grid%nnodes_loc(1)
    n2 = grid%nnodes_loc(2)
    n3 = grid%nnodes_loc(3)

    end_left(:)    = grid%npml_glob(1,:)                   ! number of PML nodes at edges z-, x-, y-
    start_right(:) = end_left(:) + grid%nmodel_glob(:) + 1 ! index number first PML node at edges z+, x+, y+

    npml_loc = 0
    do k = 1, n3
      do j = 1, n2
        do i = 1, n1

          coord_loc = (/i,j,k/)
          call grid%sgrid_loc2glob(coord_loc, coord_glob)

          if (      coord_glob(1) <= end_left(1) .or. coord_glob(1) >= start_right(1) &
              .or.  coord_glob(2) <= end_left(2) .or. coord_glob(2) >= start_right(2) &
              .or.  coord_glob(3) <= end_left(3) .or. coord_glob(3) >= start_right(3) ) then
            npml_loc = npml_loc + 1
          end if

        end do
      end do
    end do

  end subroutine cpml_init


  subroutine cpml_compute_coef_on_axis(idim, grid, dt, vmax, Rc, freq, kmax, power, a, b, k, a_half, b_half, k_half)
    integer,                         intent(in)     :: idim
    type(sgrid_type),                intent(in)     :: grid
    real,                            intent(in)     :: dt, vmax, Rc, freq, kmax
    integer,                         intent(in)     :: power
    real, dimension(:), allocatable, intent(in out) :: a, b, k, a_half, b_half, k_half
    integer, dimension(3)                           :: coord_loc, coord_glob
    real                                            :: alpha_max, d0, d0_left, d0_right
    real                                            :: d_norm, d_norm_half, Lpml_left, Lpml_right
    integer                                         :: ig_start, ig_first_mdl, ig_last_mdl, nnodes_loc, l, lg

    nnodes_loc   = grid%nnodes_loc(idim)

    call alloc_(b,      1, nnodes_loc, "b")
    call alloc_(a,      1, nnodes_loc, "a")
    call alloc_(k,      1, nnodes_loc, "k")

    call alloc_(b_half, 1, nnodes_loc, "b_half")
    call alloc_(a_half, 1, nnodes_loc, "a_half")
    call alloc_(k_half, 1, nnodes_loc, "k_half")

    ig_start     = 1 ! by convention
    ig_first_mdl = ig_start + grid%npml_glob(1,idim)
    ig_last_mdl  = ig_first_mdl + grid%nmodel_glob(idim) - 1

    Lpml_left    = real(grid%npml_glob(1,idim)) - 0.5 ! left  PML layer size
    Lpml_right   = real(grid%npml_glob(2,idim)) - 0.5 ! right PML layer size

    d0_left      = 2. * grid%npml_glob(1,idim) * grid%h(idim)
    d0_right     = 2. * grid%npml_glob(2,idim) * grid%h(idim)
    if (d0_left  > 0) d0_left  = -(power+1) * vmax * log(Rc) / d0_left
    if (d0_right > 0) d0_right = -(power+1) * vmax * log(Rc) / d0_right

    alpha_max = acos(-1.)*freq

    !----------------------------------------------------------------
    ! loop on PML grid points along idim axis to fill the coef tables
    !----------------------------------------------------------------

    coord_loc = (/1,1,1/)

    do l = 1, nnodes_loc

      coord_loc(idim) = l
      call grid%sgrid_loc2glob(coord_loc, coord_glob)
      lg = coord_glob(idim)

      if (lg < ig_first_mdl .or. lg > ig_last_mdl) then ! PML point

        ! Compute ratio between distance from the edge and PML total thickness
        if (lg < ig_first_mdl) then     ! PML point -
          d_norm      = (real(ig_first_mdl-lg) - 0.5) / real(Lpml_left) ! # x/L
          d_norm_half = (real(ig_first_mdl-lg) - 1.0) / real(Lpml_left)
          d0          = d0_left
        else! if (lg > ig_last_mdl) then ! PML point +
          d_norm      = (real(lg-ig_last_mdl) - 1.0) / real(Lpml_right)
          d_norm_half = (real(lg-ig_last_mdl) - 0.5) / real(Lpml_right)
          d0          = d0_right
        end if

        ! Compute PML coefficients on the reference grid then on the staggered grid
        call cpml_compute_coef(d_norm,      d0, power, alpha_max, kmax, dt, b(l),      a(l),      k(l))
        call cpml_compute_coef(d_norm_half, d0, power, alpha_max, kmax, dt, b_half(l), a_half(l), k_half(l))

      end if

    end do

  end subroutine cpml_compute_coef_on_axis

  
  subroutine cpml_compute_coef(d_norm, d0, power, alpha_max, kmax, dt, b, a, k)
    real,    intent(in)  :: d_norm, d0, alpha_max, kmax, dt
    integer, intent(in)  :: power
    real,    intent(out) :: a, b, k
    real                 :: alpha, d
    d     = d0 * d_norm ** power
    alpha = alpha_max * (1. - d_norm)
    k     = 1. + (kmax-1.) * d_norm ** power
    b     = exp(-(d / k + alpha) * dt)
    a     = d * (b - 1.) / (k * d + k**2 * alpha)
  end subroutine cpml_compute_coef


  subroutine alloc_wavefields()
    integer                          :: nkmax, njmax, nimax
    integer                          :: nkmin, njmin, nimin
    integer, dimension(2,3)          :: loc_range

    call grid%sgrid_allocate_field(Vx, "Vx")
    call grid%sgrid_allocate_field(Vy, "Vy")
    call grid%sgrid_allocate_field(Vz, "Vz")
    call grid%sgrid_allocate_field(P,  "P")

    ! PML memory variables
    call grid%sgrid_allocate_field(mem_Vx_x,  "mem_Vx_x")
    call grid%sgrid_allocate_field(mem_Vy_y,  "mem_Vy_y")
    call grid%sgrid_allocate_field(mem_Vz_z,  "mem_Vz_z")
    call grid%sgrid_allocate_field(mem_Px_x,  "mem_Px_x")
    call grid%sgrid_allocate_field(mem_Py_y,  "mem_Py_y")
    call grid%sgrid_allocate_field(mem_Pz_z,  "mem_Pz_z")

    ! Retrieve maximum range (because of temporal blocking optim)
    call grid%sgrid_get_valid_loc_range(1, loc_range)

    ! Bounds of the loops inside modelling domain including PML
    ! + take into account valid range considering temporal blocking
    nimin = loc_range(1,1)
    njmin = loc_range(1,2)
    nkmin = loc_range(1,3)
    nimax = loc_range(2,1)
    njmax = loc_range(2,2)
    nkmax = loc_range(2,3)

  end subroutine alloc_wavefields


  subroutine reset_wavefields()

    Vx = 0. ; Vy = 0. ; Vz = 0. ; P = 0.
    mem_Vx_x = 0. ; mem_Vy_y = 0. ; mem_Vz_z = 0. ; 
    mem_Px_x = 0. ; mem_Py_y = 0. ; mem_Pz_z = 0.

  end subroutine reset_wavefields


  subroutine free_mem()
    call dealloc_(Vx,        "Vx")
    call dealloc_(Vy,        "Vy")
    call dealloc_(Vz,        "Vz")
    call dealloc_(P,         "P")
    call dealloc_(Vp,        "Vp")
    call dealloc_(rho,       "rho")
    call dealloc_(buo_Vx,    "buo_Vx")
    call dealloc_(buo_Vy,    "buo_Vy")
    call dealloc_(buo_Vz,    "buo_Vz")
    call dealloc_(kappa,     "kappa")
    call dealloc_(mem_Vx_x,  "mem_Vx_x")
    call dealloc_(mem_Vy_y,  "mem_Vy_y")
    call dealloc_(mem_Vz_z,  "mem_Vz_z")
    call dealloc_(mem_Px_x,  "mem_Px_x")
    call dealloc_(mem_Py_y,  "mem_Py_y")
    call dealloc_(mem_Pz_z,  "mem_Pz_z")
    call dealloc_(source,    "source")
    call dealloc_(bz,        "bz")
    call dealloc_(az,        "az")
    call dealloc_(kz,        "kz")
    call dealloc_(bx,        "bx")
    call dealloc_(ax,        "ax")
    call dealloc_(kx,        "kx")
    call dealloc_(by,        "by")
    call dealloc_(ay,        "ay")
    call dealloc_(ky,        "ky")
    call dealloc_(bz_half,   "bz_half")
    call dealloc_(az_half,   "az_half")
    call dealloc_(kz_half,   "kz_half")
    call dealloc_(bx_half,   "bx_half")
    call dealloc_(ax_half,   "ax_half")
    call dealloc_(kx_half,   "kx_half")
    call dealloc_(by_half,   "by_half")
    call dealloc_(ay_half,   "ay_half")
    call dealloc_(ky_half,   "ky_half")
    call dealloc_(src_comm1, "src_comm1")
    call dealloc_(src_coor,  "src_coor")
    call dealloc_(src_loc,   "src_loc")
    call dealloc_(src_comm1, "src_comm1")
    call dealloc_(Pglob,     "Pglob")
  end subroutine free_mem


  integer function get_scheme_order()
    get_scheme_order = 4
  end function get_scheme_order


  integer function get_scheme_overlap()
    get_scheme_overlap = int(get_scheme_order()/2)
  end function get_scheme_overlap


  subroutine compute_stress()
    integer                          :: nkmax, njmax, nimax
    integer                          :: nkmin, njmin, nimin
    integer                          :: i, j, k
    real, parameter                  :: A1 = 1.125
    real, parameter                  :: A2 = -1./24.
    real                             :: A1_h1, A1_h2, A1_h3
    real                             :: A2_h1, A2_h2, A2_h3
    real                             :: dvx_dx, dvy_dy, dvz_dz
    integer, dimension(2,3)          :: loc_range

    A1_h1 = A1 / grid%h(1) ; A1_h2 = A1 / grid%h(2) ; A1_h3 = A1 / grid%h(3)
    A2_h1 = A2 / grid%h(1) ; A2_h2 = A2 / grid%h(2) ; A2_h3 = A2 / grid%h(3)

    call grid%sgrid_get_valid_loc_range(it, loc_range)

    ! Bounds of the loops inside modelling domain including PML
    ! + take into account valid range considering temporal blocking
    nimin = loc_range(1,1)
    njmin = loc_range(1,2)
    nkmin = loc_range(1,3)
    nimax = loc_range(2,1)
    njmax = loc_range(2,2)
    nkmax = loc_range(2,3)

    if (npml_loc > 0) then

      !$OMP PARALLEL NUM_THREADS(nthreads) DEFAULT(SHARED) PRIVATE(i, j, k, dvx_dx, dvy_dy, dvz_dz)
      !$OMP DO SCHEDULE(DYNAMIC)

      do k = nkmin, nkmax
        do j = njmin, njmax
          do i = nimin, nimax

            ! compute partial derivatives
            dvx_dx = A1_h2 * (Vx(i,j,k)-Vx(i,j-1,k)) + A2_h2 * (Vx(i,j+1,k)-Vx(i,j-2,k))
            dvy_dy = A1_h3 * (Vy(i,j,k)-Vy(i,j,k-1)) + A2_h3 * (Vy(i,j,k+1)-Vy(i,j,k-2))
            dvz_dz = A1_h1 * (Vz(i,j,k)-Vz(i-1,j,k)) + A2_h1 * (Vz(i+1,j,k)-Vz(i-2,j,k))

            mem_Vx_x(i,j,k) = bx(j) * mem_Vx_x(i,j,k) + ax(j) * dvx_dx
            mem_Vy_y(i,j,k) = by(k) * mem_Vy_y(i,j,k) + ay(k) * dvy_dy
            mem_Vz_z(i,j,k) = bz(i) * mem_Vz_z(i,j,k) + az(i) * dvz_dz

            ! update partial derivatives
            dvx_dx = dvx_dx + mem_Vx_x(i,j,k) 
            dvy_dy = dvy_dy + mem_Vy_y(i,j,k)
            dvz_dz = dvz_dz + mem_Vz_z(i,j,k)

            P(i,j,k) = P(i,j,k) + kappa(i,j,k) * (dvx_dx + dvy_dy + dvz_dz) * dt

          end do
        end do
      end do

      !$OMP END DO
      !$OMP END PARALLEL

    else

      !$OMP PARALLEL NUM_THREADS(nthreads) DEFAULT(SHARED) PRIVATE(i, j, k)
      !$OMP DO SCHEDULE(DYNAMIC)

      do k = nkmin, nkmax
        do j = njmin, njmax
          do i = nimin, nimax

            P(i,j,k) = P(i,j,k) &
            &           + (  A1_h2 * (Vx(i,j,k) - Vx(i,j-1,k)) + A2_h2 * (Vx(i,j+1,k) - Vx(i,j-2,k)) &
            &              + A1_h3 * (Vy(i,j,k) - Vy(i,j,k-1)) + A2_h3 * (Vy(i,j,k+1) - Vy(i,j,k-2)) &
            &              + A1_h1 * (Vz(i,j,k) - Vz(i-1,j,k)) + A2_h1 * (Vz(i+1,j,k) - Vz(i-2,j,k)) &
            &             ) * kappa(i,j,k) * dt

          end do
        end do
      end do

      !$OMP END DO
      !$OMP END PARALLEL

    end if

  end subroutine compute_stress


  subroutine compute_velocity()
    integer                          :: nkmax, njmax, nimax
    integer                          :: nkmin, njmin, nimin
    integer                          :: i, j, k
    real, parameter                  :: A1 = 1.125
    real, parameter                  :: A2 = -1./24.
    real                             :: A1_h1, A1_h2, A1_h3
    real                             :: A2_h1, A2_h2, A2_h3
    real                             :: dpx_dx, dpy_dy, dpz_dz
    integer, dimension(2,3)          :: loc_range

    A1_h1 = A1 / grid%h(1) ; A1_h2 = A1 / grid%h(2) ; A1_h3 = A1 / grid%h(3)
    A2_h1 = A2 / grid%h(1) ; A2_h2 = A2 / grid%h(2) ; A2_h3 = A2 / grid%h(3)

    call grid%sgrid_get_valid_loc_range(it, loc_range)

    ! Bounds of the loops inside modelling domain including PML
    ! + take into account valid range considering temporal blocking
    nimin = loc_range(1,1)
    njmin = loc_range(1,2)
    nkmin = loc_range(1,3)
    nimax = loc_range(2,1)
    njmax = loc_range(2,2)
    nkmax = loc_range(2,3)

    if (npml_loc > 0) then

      !$OMP PARALLEL NUM_THREADS(nthreads) DEFAULT(SHARED) PRIVATE(i, j, k, dpx_dx, dpy_dy, dpz_dz)
      !$OMP DO SCHEDULE(DYNAMIC)

      do k = nkmin, nkmax
        do j = njmin, njmax
          do i = nimin, nimax

            ! compute partial derivatives
            dpx_dx = A1_h2 * (P(i,j+1,k) - P(i,j,k)) + A2_h2 * (P(i,j+2,k) - P(i,j-1,k))
            dpy_dy = A1_h3 * (P(i,j,k+1) - P(i,j,k)) + A2_h3 * (P(i,j,k+2) - P(i,j,k-1)) 
            dpz_dz = A1_h1 * (P(i+1,j,k) - P(i,j,k)) + A2_h1 * (P(i+2,j,k) - P(i-1,j,k))

            ! update memory variables
            mem_Px_x(i,j,k) = bx_half(j) * mem_Px_x(i,j,k) + ax_half(j) * dpx_dx
            mem_Py_y(i,j,k) = by_half(k) * mem_Py_y(i,j,k) + ay_half(k) * dpy_dy
            mem_Pz_z(i,j,k) = bz_half(i) * mem_Pz_z(i,j,k) + az_half(i) * dpz_dz

            ! update partial derivatives
            dpx_dx = dpx_dx + mem_Px_x(i,j,k)
            dpy_dy = dpy_dy + mem_Py_y(i,j,k)
            dpz_dz = dpz_dz + mem_Pz_z(i,j,k)

            ! update grid point
            Vx(i,j,k) = Vx(i,j,k) + dpx_dx * buo_Vx(i,j,k) * dt 
            Vy(i,j,k) = Vy(i,j,k) + dpy_dy * buo_Vy(i,j,k) * dt 
            Vz(i,j,k) = Vz(i,j,k) + dpz_dz * buo_Vz(i,j,k) * dt  

          end do
        end do
      end do

      !$OMP END DO
      !$OMP END PARALLEL

    else

      !$OMP PARALLEL NUM_THREADS(nthreads) DEFAULT(SHARED) PRIVATE(i, j, k)
      !$OMP DO SCHEDULE(DYNAMIC)

      do k = nkmin, nkmax
        do j = njmin, njmax
          do i = nimin, nimax

            Vx(i,j,k) = Vx(i,j,k) + (A1_h2 * (P(i,j+1,k) - P(i,j,k)) + A2_h2 * (P(i,j+2,k) - P(i,j-1,k))) * buo_Vx(i,j,k) * dt 
            Vy(i,j,k) = Vy(i,j,k) + (A1_h3 * (P(i,j,k+1) - P(i,j,k)) + A2_h3 * (P(i,j,k+2) - P(i,j,k-1))) * buo_Vy(i,j,k) * dt 
            Vz(i,j,k) = Vz(i,j,k) + (A1_h1 * (P(i+1,j,k) - P(i,j,k)) + A2_h1 * (P(i+2,j,k) - P(i-1,j,k))) * buo_Vz(i,j,k) * dt  

          end do
        end do
      end do

      !$OMP END DO
      !$OMP END PARALLEL

    end if

  end subroutine compute_velocity


  subroutine print_starting_banner()
    if (myid_world == 0) then    
      write(*,*) ''   
      write(*,*) '======================================================================'
      write(*,*) ''
      write(*,*) '                        3D Domain Decomposition Test'
      write(*,*) '' 
      write(*,*) '======================================================================'
      write(*,*) ''
      call comm%print_info()
      write(*,*) ' Number of threads :             ', nthreads
    end if
    end subroutine print_starting_banner


  subroutine print_successfully_program_ended_banner()
    if (myid_world == 0) then
      write(*,*) ''
      write(*,*) '======================================================================'
      write(*,*) ''
      write(*,*) '     End of the program                        '
      write(*,*) ''
      write(*,*) '======================================================================'
    end if
  end subroutine print_successfully_program_ended_banner


  subroutine fdm_write_vtk_legacy_fmt(grid, isrc_glob, it, F, fieldname)
    type(sgrid_type),                    intent(in) :: grid
    integer,                             intent(in) :: isrc_glob
    integer,                             intent(in) :: it
    real, dimension(:,:,:), allocatable, intent(in) :: F
    character(len=*),                    intent(in) :: fieldname
    integer                                         :: ierr
    integer                                         :: nymax, nxmax, nzmax
    integer                                         :: fileunit = 11
    character                                       :: filename*80
    character                                       :: itstr*6, myidstr*6, isrc_glob_str*6 
    character                                       :: istr1*8, istr2*8, istr3*8, fstr1*32, fstr2*32, fstr3*32
    character                                       :: buffer*120, ntotstr*12
    character(len=*),                     parameter :: end_of_line = char(10)
    integer                                         :: ix, iy, iz

    write(itstr,        '(I6)' ) it
    write(myidstr,      '(I6)' ) myid
    write(isrc_glob_str,'(I6)' ) isrc_glob
    write(ntotstr(1:12),'(I12)') product(grid%n_glob)

    filename = 'output' // '_bin'                                 &
    &                   // '_src' // trim(adjustl(isrc_glob_str)) &
    &                   // '_it'  // trim(adjustl(itstr))         &
    &                   // '.vtk'

    open(unit = fileunit, file = trim(filename), status='replace', access='STREAM', convert='big_endian', iostat=ierr)
    if (ierr /= 0) then
      write(*,*) " ERROR : could not open file : ", filename, " unit = ", fileunit
      stop
    endif

    ! HEADER SECTION
    buffer = '# vtk DataFile Version 3.0'
    write(fileunit) trim(buffer//end_of_line)
    buffer = 'vtk output'
    write(fileunit) trim(buffer//end_of_line)
    buffer = 'BINARY'
    write(fileunit) trim(buffer//end_of_line)
    buffer = 'DATASET STRUCTURED_POINTS'
    write(fileunit) trim(buffer//end_of_line)

    ! GRID SECTION (grid dimensions includes PML)
    write(istr1(1:8),'(i8)') grid%n_glob(1)
    write(istr2(1:8),'(i8)') grid%n_glob(2)
    write(istr3(1:8),'(i8)') grid%n_glob(3)
    buffer = 'DIMENSIONS '//istr1//' '//istr2//' '//istr3
    write(fileunit) trim(buffer//end_of_line)
    write(fstr1(1:32),'(f14.7)') grid%bbox_glob(1,1)
    write(fstr2(1:32),'(f14.7)') grid%bbox_glob(1,2)
    write(fstr3(1:32),'(f14.7)') grid%bbox_glob(1,3)
    buffer = 'ORIGIN '//fstr1//' '//fstr2//' '//fstr3
    write(fileunit) trim(buffer//end_of_line)
    write(fstr1(1:32),'(f14.7)') grid%h(1)
    write(fstr2(1:32),'(f14.7)') grid%h(2)
    write(fstr3(1:32),'(f14.7)') grid%h(3)
    buffer = 'SPACING '//fstr1//' '//fstr2//' '//fstr3
    write(fileunit) trim(buffer//end_of_line)

    ! POINT DATA SECTION
    buffer = 'POINT_DATA '//ntotstr
    write(fileunit) trim(buffer//end_of_line)

    nxmax = grid%n_glob(1)
    nymax = grid%n_glob(2)
    nzmax = grid%n_glob(3)

    ! Write the look-up table associated to the field
    buffer = 'SCALARS ' // trim(adjustl(fieldname)) // ' float'
    write(fileunit) trim(buffer//end_of_line)
    buffer = 'LOOKUP_TABLE default'
    write(fileunit) trim(buffer//end_of_line)
    write(fileunit) (((F(ix,iy,iz), ix = 1, nxmax), iy = 1, nymax), iz = 1, nzmax)

    close(fileunit)

  end subroutine fdm_write_vtk_legacy_fmt


  subroutine print_statistics()
    real    :: valmin, valmax
    integer :: i

    if (myid == 0) then
      write(*,'(56A1)') ('-', i=1, 56)
      write(*,*) ' Statistics:'
    end if

    valmin = comm%sg_comm_get_field_glob_min_value(grid, Vx)
    valmax = comm%sg_comm_get_field_glob_max_value(grid, Vx)
    if (myid == 0) write(*,*) ' Vx     : min = ', valmin, ' max = ', valmax

    valmin = comm%sg_comm_get_field_glob_min_value(grid, Vy)
    valmax = comm%sg_comm_get_field_glob_max_value(grid, Vy)
    if (myid == 0) write(*,*) ' Vy     : min = ', valmin, ' max = ', valmax

    valmin = comm%sg_comm_get_field_glob_min_value(grid, Vz)
    valmax = comm%sg_comm_get_field_glob_max_value(grid, Vz)
    if (myid == 0) write(*,*) ' Vz     : min = ', valmin, ' max = ', valmax

    valmin = comm%sg_comm_get_field_glob_min_value(grid, P)
    valmax = comm%sg_comm_get_field_glob_max_value(grid, P)
    if (myid == 0) write(*,*) ' P      : min = ', valmin, ' max = ', valmax
    if (myid == 0) write(*,'(56A1)') ('-', i=1, 56)

  end subroutine print_statistics


end program test_dd_sg_ac_iso_o4
